x-logging:
      &default-logging
      driver: "json-file"
      options:
          max-size: "200k"
          max-file: "10"

# x-logging: &x-logging
#   logging:
#     driver: loki
#     options:
#       loki-url: "http://loki:3100/loki/api/v1/push"
#       loki-retries: "5"
#       loki-batch-size: "400"
#       loki-external-labels: service={{.Name}}
#   labels:
#     logging: "promtail"
#     logging_jobname: "containerlogs"

# x-common: &x-common
#   <<: *x-logging
#   privileged: false
#   volumes:
#     - /etc/localtime:/etc/localtime:ro
#   security_opt:
#     - no-new-privileges=true
#   tmpfs:
#     - /tmp:rw,noexec,nosuid,size=32m
#   ulimits:
#     nproc: 6144
#     nofile:
#       soft: 6000
#       hard: 12000

# <<: *x-common

networks:
  observability:
    # external: true
    driver: bridge
    name: observability
    ipam:
      config:
        - subnet: 172.16.10.0/24
  dockerproxynet:
   name: dockerproxynet
   ipam:
    config:
      - subnet: 172.16.11.0/24

volumes:
    prometheus_data: 
    grafana_data: 
    alertmanager_data: 
    loki_data: 
    promtail_data: 
    postgres_data:
    minio_data: 
    victoriametrics_data:
    victorialogs_data:
    vmagent_data:
    angie_data:
    log-generator_data:
    flog_data:
    portainer_data:

services:

  prometheus:
    image: prom/prometheus:v2.52.0
    container_name: prometheus
    privileged: true
    user: root
    volumes:
      - ./etc/prometheus:/etc/prometheus:ro
      - prometheus_data:/var/lib/prometheus
      - ./etc/alertmanager:/etc/alertmanager:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/var/lib/prometheus'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    restart: always
    ports:
      - 9090:9090
    networks:
      - observability
    logging: *default-logging  

  nodeexporter:
    image: prom/node-exporter:v1.8.1
    container_name: nodeexporter
    privileged: true
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
      - '--no-collector.rapl'
    restart: always
    ports:
      - 9100:9100
    networks:
      - observability
    logging: *default-logging  

  alertmanager:
    image: prom/alertmanager:v0.27.0
    container_name: alertmanager
    user: root
    volumes:
      - alertmanager_data:/var/lib/alertmanager
      - ./etc/alertmanager:/etc/alertmanager:ro
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/var/lib/alertmanager'
      - '--cluster.listen-address=:9094'
    restart: always
    ports:
      - 9094:9094
      - 9093:9093
    networks:
      - observability
    logging: *default-logging  

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.49.1
    container_name: cadvisor
    privileged: true
    devices:
      - /dev/kmsg:/dev/kmsg
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /sys/fs/cgroup:/cgroup:ro
      - /dev/disk/:/dev/disk:ro
    command: 
      - '--port=9089'
      - '--storage_duration=10m0s'
      - '--docker_only=true'
      - '--disable_metrics=advtcp,app,cpu_topology,cpuset,disk,hugetlb,memory_numa,percpu,perf_event,referenced_memory,resctrl,sched,tcp,udp'
    restart: always
    ports:
      - 9089:9089
    networks:
      - observability
    logging: *default-logging 

  loki:
    image: grafana/loki:3.0.0
    container_name: loki
    user: root
    volumes:
      - ./etc/loki:/etc/loki:ro
      - loki_data:/var/lib/loki
    command: 
      - '-config.file=/etc/loki/config.yml'
      - '-config.expand-env=true'
    ports:
      - 3100:3100
    restart: always
    networks:
      - observability
    logging: *default-logging 

  promtail:
    image: grafana/promtail:3.0.0
    container_name: promtail
    privileged: true
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /var/log:/var/log:ro
      - ./etc/promtail:/etc/promtail:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - promtail_data:/tmp/promtail
      - flog_data:/var/log/flog:ro
      - log-generator_data:/var/log/log-generator-apache:ro
    command: 
      - '-config.file=/etc/promtail/config.yml'
    restart: always
    ports:
      - 9080:9080
    networks:
      - observability
    logging: *default-logging 

  # vmagent:
  #   container_name: vmagent
  #   image: victoriametrics/vmagent:v1.101.0
  #   ports:
  #     - 8429:8429
  #   volumes:
  #     - vmagent_data:/var/lib/vmagent
  #     - ./etc/vmagent:/etc/vmagent:ro
  #     - ./etc/prometheus:/etc/prometheus:ro
  #   command:
  #     # - '-config.file /etc/vmagent/vmagent.yml'
  #     - '--promscrape.config=/etc/victoriametrics/victoriametrics.yml'
  #     - '--promscrape.config.strictParse=false'
  #     - '--remoteWrite.tmpDataPath=/var/lib/vmagent'
  #     - '--promscrape.configCheckInterval=1m'
  #     - '--httpListenAddr=:8429'
  #     - '--remoteWrite.maxDiskUsagePerURL=128MB'
  #     - '--remoteWrite.url='
  #   networks:
  #     - observability
  #   restart: always
      # logging: *default-logging 

  victoriametrics:
    container_name: victoriametrics
    image: victoriametrics/victoria-metrics:v1.101.0
    ports:
      - 8428:8428
    volumes:
      - victoriametrics_data:/var/lib/victoriametrics
      # - ./etc/victoriametrics:/etc/victoriametrics:ro
      - ./etc/prometheus/prometheus.yml:/etc/victoriametrics/victoriametrics.yml:ro
      - ./etc/alertmanager:/etc/alertmanager:ro
    command:
      - "--storageDataPath=/var/lib/victoriametrics"
      - '--promscrape.config=/etc/victoriametrics/victoriametrics.yml'
      - '--promscrape.config.strictParse=false'
      - '--retentionPeriod=15d'
      - "--httpListenAddr=:8428"
      - "--vmalert.proxyURL=http://vmalert:8880"
      - '--promscrape.configCheckInterval=1m'
    networks:
      - observability
    restart: always
    logging: *default-logging     

  victorialogs:
    container_name: victorialogs
    image: docker.io/victoriametrics/victoria-logs:v0.19.0-victorialogs
    command:
      - '--storageDataPath=/var/lib/victorialogs'
      - '--loggerFormat=json'
      - '--httpListenAddr=:9428'
    volumes:
      - victorialogs_data:/var/lib/victorialogs
      - ./etc/victorialogs:/etc/victorialogs:ro
    ports:
      - 9428:9428
    networks:
      - observability
    logging: *default-logging 

  grafana:
    image: grafana/grafana:11.0.0
    container_name: grafana
    entrypoint: [ "/bin/bash", "-c" ]
    command: [ "/etc/grafana/download.sh && /run.sh" ]
    volumes:
      - grafana_data:/var/lib/grafana
      - ./etc/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./etc/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      # - ./var/lib/grafana/plugins:/var/lib/grafana/plugins
      - ./var/lib/grafana/dashboards:/var/lib/grafana/dashboards
      - ./etc/grafana/download.sh:/etc/grafana/download.sh
    restart: always
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_DEFAULT_INSTANCE_NAME=localhost
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ENABLE_GZIP=true
      - GF_PATHS_DATA=/var/lib/grafana
      - GF_SERVER_ROOT_URL=http://localhost:3000
      - GRAFANA_PORT=3000
      - GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS=victorialogs-datasource,victoriametrics-datasource
    ports:
      - 3000:3000
    networks:
      - observability
    logging: *default-logging

  watchtower:
    image: containrrr/watchtower:1.7.1
    container_name: watchtower
    restart: unless-stopped
    networks:
      - observability
    env_file:
      - path: ./.env
        required: true # default
      - path: ./.env.override
        required: false  
    environment:
      # WATCHTOWER_POLL_INTERVAL: 21600
      TZ: Europe/Moscow
      WATCHTOWER_SCHEDULE: "0 0 4 * * *"
      WATCHTOWER_CLEANUP: true
      WATCHTOWER_REMOVE_VOLUMES: "false"
      WATCHTOWER_INCLUDE_STOPPED: "true"
      WATCHTOWER_INCLUDE_RESTARTING: "true"
      WATCHTOWER_MONITOR_ONLY: 'false'
      WATCHTOWER_LIFECYCLE_HOOKS: "true"
      WATCHTOWER_HTTP_API_METRICS: "true"
      WATCHTOWER_HTTP_API_TOKEN: "demotoken"
      WATCHTOWER_HTTP_API_UPDATE: "true"
      WATCHTOWER_ROLLING_RESTART: "true"
      WATCHTOWER_NOTIFICATIONS: shoutrrr
      WATCHTOWER_DEBUG: "true"
      WATCHTOWER_NOTIFICATION_REPORT: "true"
      WATCHTOWER_NOTIFICATION_URL: $WATCHTOWER_TELEGRAM_URL 
      #"telegram://$telegrambottoken@telegram/?channels=$telegramchatid"
    command: --debug  --http-api-update
    ports:
      - 8088:8080
    volumes:
      - ${SOCK_PATH:-/var/run/docker.sock}:/var/run/docker.sock
      # - "${PRIMARY_MOUNT}/watchtower/config/:/config"
      # - "${PRIMARY_MOUNT}/watchtower/docker-config.json:/config.json"
    logging: *default-logging 

  # tailscale:
  #   privileged: true
  #   hostname: tailscale                                         # This will become the tailscale device name
  #   network_mode: "host"
  #   container_name: tailscale
  #   image: tailscale/tailscale:v1.66.4
  #   volumes:
  #     - "/opt/appdata/tailscale/var_lib:/var/lib"        # State data will be stored in this directory
  #     - "/dev/net/tun:/dev/net/tun"                      # Required for tailscale to work
  #   cap_add:                                               # Required for tailscale to work
  #     - net_admin
  #     - sys_module
  # networks:
  #     - observability
  #   command: tailscaled
    # environment:
    #     - TS_AUTHKEY=bvshbvjhsrbhvjdbhjdvbjhdbjv
    #     - TS_USERSPACE=true
    #     - TS_STATE_DIR=/var/lib/tailscale
    #     - TS_SOCKET=/var/run/tailscale/tailscaled.sock
          # '- TS_HOSTNAME=${TS_HOSTNAME}
  #   restart: unless-stopped
    # logging: *default-logging 

  # zerotier:
  #   image: zerotier/zerotier:1.14.0
   #   container_name: zerotier
  #   restart: always
  #   volumes:
  #     - ./etc/zerotier/zerotier-one:/var/lib/zerotier-one
  #     - ./etc/zerotier/config:/data/config
  #     - ./service.d/zerotier:/service.d
  #   cap_drop:
  #     - NET_RAW
  #     - NET_ADMIN
  #     - SYS_ADMIN
  #   devices:
    # environment:
      # - ZEROTIER_API_SECRET=${ZEROTIER_API_SECRET}
      # - ZEROTIER_IDENTITY_PUBLIC=${ZEROTIER_IDENTITY_PUBLIC}
      # - ZEROTIER_IDENTITY_SECRET=${ZEROTIER_IDENTITY_SECRET}
  #     - /dev/net/tun
  #   privileged: true
  #   network_mode: host
  # networks:
  #     - observability
    # logging: *default-logging 

  # vmalert:
  #   container_name: vmalert
  #   image: victoriametrics/vmalert:v1.101.0
  #   depends_on:
  #     - "victoriametrics"
  #     - "alertmanager"
  #   ports:
  #     - 8880:8880
  #   volumes:
  #     - ./alerts.yml:/etc/alerts/alerts.yml
  #     - ./alerts-health.yml:/etc/alerts/alerts-health.yml
  #     - ./alerts-vmagent.yml:/etc/alerts/alerts-vmagent.yml
  #     - ./alerts-vmalert.yml:/etc/alerts/alerts-vmalert.yml
  #   command:
  #     - "--datasource.url=http://victoriametrics:8428/"
  #     - "--remoteRead.url=http://victoriametrics:8428/"
  #     - "--remoteWrite.url=http://victoriametrics:8428/"
  #     - "--notifier.url=http://alertmanager:9093/"
  #     - "--rule=/etc/alerts/*.yml"
  #     # display source of alerts in grafana
  #     - "--external.url=http://127.0.0.1:3000" #grafana outside container
  #     # when copypaste the line be aware of '$$' for escaping in '$expr'
  #     - '--external.alert.source=explore?orgId=1&left={"datasource":"VictoriaMetrics","queries":[{"expr":{{$$expr|jsonEscape|queryEscape}},"refId":"A"}],"range":{"from":"now-1h","to":"now"}}'
  #   networks:
  #     - observability
  #   restart: always
    # logging: *default-logging 

  pushgateway:
    image: prom/pushgateway:v1.9.0
    container_name: pushgateway
    restart: always
    ports:
      - 9091:9091
    networks:
      - observability
    logging: *default-logging 

  # caddy:
  #   image: caddy:2.3.0
  #   container_name: caddy
  #   ports:
  #     - "3000:3000"
  #     - "9090:9090"
  #     - "9093:9093"
  #     - "9091:9091"
  #     - "3100:3100"
  #     - 9115:9115
  #     - 9001:9001
  #     - 9000:9000
  #     - 9428:9428
  #     - 9089:9089
  #     - 8429:8429
  #     - 9080:9080
  #     - 9100:9100
  #     - '8088:8080'
  #   volumes:
  #     - ./caddy:/etc/caddy
  #   environment:
  #     - ADMIN_USER=${ADMIN_USER:-admin}
  #     - ADMIN_PASSWORD=${ADMIN_PASSWORD:-admin}
  #     - ADMIN_PASSWORD_HASH=${ADMIN_PASSWORD_HASH:-JDJhJDE0JE91S1FrN0Z0VEsyWmhrQVpON1VzdHVLSDkyWHdsN0xNbEZYdnNIZm1pb2d1blg4Y09mL0ZP}
  #   restart: always
  #   networks:
    #   - observability
    # logging: *default-logging 

  # certbot:
  #   image: certbot/certbot:v2.10.0
  #   container_name: certbot
  #   volumes:
  #     - ./nginx/certbot/conf:/etc/letsencrypt
  #     - ./nginx/certbot/www:/var/www/certbot
  #   command: certonly --webroot --webroot-path=/var/www/certbot --email your_email@example.com --agree-tos --no-eff-email -d your_domain.com

  angie_init_htpasswd:
    image: mhenry07/apache2-utils
    container_name: angie_init_htpasswd
    env_file:
      - path: ./.env
        required: true # default
      - path: ./.env.override
        required: false
    command: sh -c "[ -f /etc/angie_data/.htpasswd ] && { echo 'File /etc/angie_data/.htpasswd exists'; exit 1; } || mkdir -p /etc/angie_data && htpasswd -Bbc /etc/angie_data/.htpasswd ${angieadmin:-admin} ${angiepassword:-admin}"
    volumes:
      - angie_data:/etc/angie_data
    logging: *default-logging

  angie_init_cert:
    image: runalsh/angie:1.5.2
    container_name: angie_init_cert
    command: sh -c "[ -f /etc/angie_data/privkey.pem ] && { echo 'File /etc/angie_data/privkey.pem exists'; exit 1; } || mkdir -p /etc/angie_data && apk add --no-cache openssl && openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout /etc/angie_data/privkey.pem -out /etc/angie_data/certificate.pem -subj '/CN=your_domain.com'"
    volumes:
      - angie_data:/etc/angie_data
    logging: *default-logging

  angie:
    image: runalsh/angie:1.5.2
    container_name: angie
    depends_on:
      - angie_init_htpasswd
      - angie_init_cert
    ports:
      - 80:80
      - 443:443
    volumes:
      - ./etc/angie/angie.conf:/etc/angie/angie.conf:ro
      - angie_data:/etc/angie_data:ro
    restart: always
    networks:
      - observability
    logging: *default-logging

  minio:
    image: minio/minio:RELEASE.2024-05-28T17-19-04Z
    container_name: minio
    volumes:
      - minio_data:/data
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=admin123
      - MINIO_PROMETHEUS_AUTH_TYPE="public"
      - MINIO_LOGGER_WEBHOOK_ENABLE_target1="on"
      - MINIO_LOGGER_WEBHOOK_AUTH_TOKEN_target1="token"
      - MINIO_LOGGER_WEBHOOK_ENDPOINT_target1=http://minio:8080/minio/logs
      - MINIO_COMPRESSION_ENABLE="on"
      - MINIO_COMPRESSION_EXTENSIONS=".txt,.log,.csv,.json,.tar,.xml,.bin"
      - MINIO_COMPRESSION_MIME_TYPES="text/*,application/json,application/xml"
      - MINIO_ACCESS_KEY=s3accesskey
      - MINIO_SECRET_KEY=s3accesskey
    entrypoint:
      - sh
      - -euc
      - |
        mkdir -p /data/loki-data && \
        mkdir -p /data/loki-ruler && \
        minio server /data --console-address ":9001"
    restart: unless-stopped
    ports:
      - 9000:9000
      - 9001:9001
    networks:
      - observability
    logging: *default-logging 

  # keycloak:
  #   image: keycloak/keycloak:24.0.4
  #   container_name: keycloak
  #   environment:
  #     KEYCLOAK_ADMIN: admin
  #     KEYCLOAK_ADMIN_PASSWORD: admin
  #     DB_VENDOR: postgres
  #     DB_ADDR: postgres
  #     DB_DATABASE: keycloak
  #     DB_USER: keycloak
  #     DB_PASSWORD: password
  #     KC_HEALTH_ENABLED: 'true'
  #     KC_METRICS_ENABLED: 'true'
  #   command: "start-dev"  
  #   ports:
  #     - '8088:8080'
  #   restart: always
  #   networks:
  #     - observability      
    # logging: *default-logging 

  # postgres:
  #   container_name: postgres
  #   image: postgres:16.3-alpine
  #   environment:
  #     POSTGRES_DB: keycloak
  #     POSTGRES_USER: keycloak
  #     POSTGRES_PASSWORD: password
  #   volumes:
  #     - postgres_data:/var/lib/postgresql/data
  #   ports:
  #     - '5432:5432'
  #   networks:
  #     - observability
  #   restart: always
    # logging: *default-logging 

  blackbox-exporter:
    image: bitnami/blackbox-exporter:0.25.0
    volumes:
      - ./etc/blackbox:/etc/blackbox:ro
    command:
      - '--config.file=/etc/blackbox/blackbox.yml'
    ports:
      - '9115:9115'
    restart: always
    networks:
      - observability
    logging: *default-logging 

  log-generator-apache:
    build:
      context: ./log-generator
      dockerfile: Dockerfile
      tags:
        - "runalsh/log-generator:latest"
    volumes:
      - ./log-generator:/var/log-generator:ro
      - log-generator_data:/var/log/log-generator-apache
    networks:
      - observability    
    logging: *default-logging

  flog:
    image: mingrammer/flog:0.4.3
    command:
      - --loop
      - --format=json
      - --number=10
      - --delay=100ms
      - --output=/var/log/flog/generated-logs.txt
      - --overwrite
      - --type=log
    networks:
      - observability
    volumes:
      - flog_data:/var/log/flog
    # logging:
    #   driver: fluentd
    #   options:
    #     mode: non-blocking
    #     fluentd-async-connect: "true"
    #     fluentd-sub-second-precision: "true"

  # telemetrygen:
  #   build:
  #     context: ../../../cmd/telemetrygen/
  #   command:
  #     - traces
  #     - --otlp-endpoint=otecollector:4317
  #     - --otlp-insecure
  #     - --rate=100
  #     - --duration=10h

  dockerproxy:
    image: tecnativa/docker-socket-proxy:0.1.2
    restart: always
    container_name: dockerproxy
    networks:
      - dockerproxynet
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      LOG_LEVEL: info
      # 0 to revoke access / 1 to grant access.
      ## Granted by Default
      EVENTS: 1
      PING: 1
      VERSION: 1
      ## Revoked by Default
      # Security critical
      AUTH: 0
      SECRETS: 0
      # Not always needed
      BUILD: 0
      COMMIT: 0
      CONFIGS: 0
      CONTAINERS: 1 # Traefik, portainer, etc.
      DISTRIBUTION: 0
      EXEC: 0
      IMAGES: 1 # Portainer
      INFO: 1 # Portainer
      NETWORKS: 1 # Portainer
      NODES: 0
      PLUGINS: 0
      SERVICES: 1 # Portainer
      SESSION: 0
      SWARM: 0
      SYSTEM: 0
      TASKS: 1 # Portainer
      VOLUMES: 1 # Portainer

  portainer:
    image: portainer/portainer-ce:2.20.3-alpine
    restart: unless-stopped
    container_name: portainer
    networks:
      - dockerproxynet
    command: ["-H", "tcp://dockerproxy:2375", "--tlsskipverify", "--admin-password", "$$2y$$05$$kvClGmhatZIlunYz332q2ebJKEFSeEdwwRL2BDnbfebJvOHC7GUTy"] #admin : admin
    healthcheck:
      test: "wget --no-verbose --tries=1 --spider --no-check-certificate http://localhost:9000 || exit 1"
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 20s
    ports:
      - 9000:9000
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - portainer_data:/data
    depends_on:
      - dockerproxy

































