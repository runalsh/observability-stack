include:
  - docker-compose.agents.yml
  - docker-compose.override.yml
#  - ./opensearch/docker-compose.opensearch.yaml
# - ./elasticsearch/docker-compose.elasticsearch.yaml

x-logging:
      &default-logging
      driver: "json-file"
      options:
          max-size: "200k"
          max-file: "10"

# x-logging: &x-logging
#   logging:
#     driver: loki
#     options:
#       loki-url: "http://loki:3100/loki/api/v1/push"
#       loki-retries: "5"
#       loki-batch-size: "400"
#       loki-external-labels: service={{.Name}}
#   labels:
#     logging: "promtail"
#     logging_jobname: "containerlogs"

# x-common: &x-common
#   <<: *x-logging
#   privileged: false
#   volumes:
#     - /etc/localtime:/etc/localtime:ro
#   security_opt:
#     - no-new-privileges=true
#   tmpfs:
#     - /tmp:rw,noexec,nosuid,size=32m
#   ulimits:
#     nproc: 6144
#     nofile:
#       soft: 6000
#       hard: 12000

# <<: *x-common

networks:
  observability:
    # external: true
    driver: bridge
    name: observability
    ipam:
      config:
        - subnet: 172.16.10.0/24

volumes:
    prometheus_data: 
    grafana_data: 
    alertmanager_data: 
    loki_data: 
    promtail_data: 
    postgres_data:
    minio_data: 
    victoriametrics_data:
    victorialogs_data:
    vmagent_data:
    angie_data:
    log-generator_data:
    flog_data:
    portainer_data:
    zerotier_data:
    tailscale_data:
    influx_data:
    clickhouse_data:
    mimir_data:

services:

  prometheus:
    image: prom/prometheus:v3.1.0
    container_name: prometheus
    privileged: true
    user: root
    volumes:
      - ./etc/prometheus:/etc/prometheus:ro
      - prometheus_data:/var/lib/prometheus
      - ./etc/alertmanager:/etc/alertmanager:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/var/lib/prometheus'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--storage.tsdb.retention.time=15d'
      - '--storage.tsdb.min-block-duration=1m' #for testing thanos , recommend 2h
      - '--storage.tsdb.max-block-duration=1m' #for testing thanos , recommend 2h
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    restart: always
    ports:
      - 9090:9090
    networks:
      - observability
    logging: *default-logging
    deploy:
      replicas: $prometheus 

  nodeexporter:
    image: prom/node-exporter:v1.8.1
    container_name: nodeexporter
    privileged: true
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
      - '--no-collector.rapl'
    restart: always
    ports:
      - 9100:9100
    networks:
      - observability
    logging: *default-logging
    deploy:
      replicas: $nodeexporter

  alertmanager:
    image: prom/alertmanager:v0.28.0
    container_name: alertmanager
    user: root
    volumes:
      - alertmanager_data:/var/lib/alertmanager
      - ./etc/alertmanager:/etc/alertmanager:ro
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/var/lib/alertmanager'
      - '--cluster.listen-address=:9094'
    restart: always
    ports:
      - 9094:9094
      - 9093:9093
    networks:
      - observability
    logging: *default-logging
    deploy:
      replicas: $alertmanager  

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.51.0
    container_name: cadvisor
    privileged: true
    devices:
      - /dev/kmsg:/dev/kmsg
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /sys/fs/cgroup:/cgroup:ro
      - /dev/disk/:/dev/disk:ro
    command: 
      - '--port=9089'
      - '--storage_duration=10m0s'
      - '--docker_only=true'
      - '--disable_metrics=advtcp,app,cpu_topology,cpuset,disk,hugetlb,memory_numa,percpu,perf_event,referenced_memory,resctrl,sched,tcp,udp'
    restart: always
    ports:
      - 9089:9089
    networks:
      - observability
    logging: *default-logging
    deploy:
      replicas: $cadvisor

  loki:
    image: grafana/loki:3.3.2
    container_name: loki
    user: root
    volumes:
      - ./etc/loki:/etc/loki:ro
      - loki_data:/var/lib/loki
    command: 
      - '-config.file=/etc/loki/config.yml'
      - '-config.expand-env=true'
    ports:
      - 3100:3100
    restart: always
    networks:
      - observability
    logging: *default-logging 
    deploy:
      replicas: $loki

  promtail:
    image: grafana/promtail:3.3.2
    container_name: promtail
    privileged: true
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /var/log:/var/log:ro
      - ./etc/promtail:/etc/promtail:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - promtail_data:/tmp/promtail
      - flog_data:/var/log/flog:ro
      - log-generator_data:/var/log/log-generator-apache:ro
    command: 
      - '-config.file=/etc/promtail/config.yml'
    restart: always
    ports:
      - 9080:9080
    networks:
      - observability
    logging: *default-logging 
    deploy:
      replicas: $promtail

  vmagent:
    container_name: vmagent
    image: victoriametrics/vmagent:v1.111.0
    ports:
      - 8429:8429
      - 2003:2003
      - 2003:2003/udp
    volumes:
      - vmagent_data:/var/lib/vmagent
      - ./etc/vmagent:/etc/vmagent:ro
      - ./etc/prometheus:/etc/prometheus:ro
    command:
      # - '-config.file /etc/vmagent/vmagent.yml'
      - '--promscrape.config=/etc/victoriametrics/victoriametrics.yml'
      - '--promscrape.config.strictParse=false'
      - '--remoteWrite.tmpDataPath=/var/lib/vmagent'
      - '--promscrape.configCheckInterval=1m'
      - '--httpListenAddr=:8429'
      - '--remoteWrite.maxDiskUsagePerURL=128MB'
      - "--remoteWrite.url=http://victoriametrics:8428/api/v1/write"
      - "-graphiteListenAddr=:2003"
      - "-remoteWrite.relabelConfig=/etc/vmagent/relabel.yml"
    networks:
      - observability
    restart: always
    logging: *default-logging
    deploy:
      replicas: $vmagent 

  telegraf:
    image: telegraf:1.33.0-alpine
    container_name: telegraf
    volumes:
      - ./etc/telegraf/telegraf.conf:/etc/telegraf/telegraf.conf:ro
    depends_on:
      - victoriametrics
    ports:
      - 2004:2004
    networks:
      - observability
    deploy:
      replicas: $telegraf  

  victoriametrics:
    container_name: victoriametrics
    image: victoriametrics/victoria-metrics:v1.111.0
    ports:
      - 8428:8428
    volumes:
      - victoriametrics_data:/var/lib/victoriametrics
      # - ./etc/victoriametrics:/etc/victoriametrics:ro
      - ./etc/prometheus/prometheus.yml:/etc/victoriametrics/victoriametrics.yml:ro
      - ./etc/alertmanager:/etc/alertmanager:ro
    command:
      - "--storageDataPath=/var/lib/victoriametrics"
      - '--promscrape.config=/etc/victoriametrics/victoriametrics.yml'
      - '--promscrape.config.strictParse=false'
      - '--retentionPeriod=15d'
      - "--httpListenAddr=:8428"
      - "--vmalert.proxyURL=http://vmalert:8880"
      - '--promscrape.configCheckInterval=1m'
    networks:
      - observability
    restart: always
    logging: *default-logging     
    deploy:
      replicas: $victoriametrics

  victorialogs:
    container_name: victorialogs
    image: docker.io/victoriametrics/victoria-logs:v1.9.1-victorialogs
    command:
      - '--storageDataPath=/var/lib/victorialogs'
      - '--loggerFormat=json'
      - '--httpListenAddr=:9428'
    volumes:
      - victorialogs_data:/var/lib/victorialogs
      - ./etc/victorialogs:/etc/victorialogs:ro
    ports:
      - 9428:9428
    networks:
      - observability
    logging: *default-logging 
    deploy:
      replicas: $victorialogs

  grafana:
    image: grafana/grafana:11.5.0
    container_name: grafana
    entrypoint: [ "/bin/bash", "-c" ]
    command: [ "/etc/grafana/download.sh && /run.sh" ]
    volumes:
      - grafana_data:/var/lib/grafana
      - ./etc/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./etc/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      # - ./var/lib/grafana/plugins:/var/lib/grafana/plugins
      - ./var/lib/grafana/dashboards:/var/lib/grafana/dashboards
      - ./etc/grafana/download.sh:/etc/grafana/download.sh
    restart: always
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_DEFAULT_INSTANCE_NAME=localhost
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ENABLE_GZIP=true
      - GF_EXPLORE_ENABLED=true
      - GF_PATHS_DATA=/var/lib/grafana
      - GF_SERVER_ROOT_URL=http://localhost:3000
      - GRAFANA_PORT=3000
      - GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS=victorialogs-datasource,victoriametrics-datasource
    ports:
      - 3000:3000
    networks:
      - observability
    logging: *default-logging
    deploy:
      replicas: $grafana

  watchtower:
    image: containrrr/watchtower:1.7.1
    container_name: watchtower
    restart: unless-stopped
    networks:
      - observability
    env_file:
      - path: ./.env
        required: true # default
      - path: ./.env.override
        required: false  
    environment:
      # WATCHTOWER_POLL_INTERVAL: 21600
      TZ: Europe/Moscow
      WATCHTOWER_SCHEDULE: "0 0 4 * * *"
      WATCHTOWER_CLEANUP: true
      WATCHTOWER_REMOVE_VOLUMES: "false"
      WATCHTOWER_INCLUDE_STOPPED: "true"
      WATCHTOWER_INCLUDE_RESTARTING: "true"
      WATCHTOWER_MONITOR_ONLY: 'false'
      WATCHTOWER_LIFECYCLE_HOOKS: "true"
      WATCHTOWER_HTTP_API_METRICS: "true"
      WATCHTOWER_HTTP_API_TOKEN: "demotoken"
      WATCHTOWER_HTTP_API_UPDATE: "true"
      WATCHTOWER_ROLLING_RESTART: "true"
      WATCHTOWER_NOTIFICATIONS: shoutrrr
      WATCHTOWER_DEBUG: "true"
      WATCHTOWER_NOTIFICATION_REPORT: "true"
      WATCHTOWER_NOTIFICATION_URL: $WATCHTOWER_TELEGRAM_URL 
      #"telegram://$telegrambottoken@telegram/?channels=$telegramchatid"
    command: --debug  --http-api-update
    ports:
      - 8088:8080
    volumes:
      - ${SOCK_PATH:-/var/run/docker.sock}:/var/run/docker.sock
      # - "${PRIMARY_MOUNT}/watchtower/config/:/config"
      # - "${PRIMARY_MOUNT}/watchtower/docker-config.json:/config.json"
    logging: *default-logging 
    deploy:
      replicas: $watchtower
    
  whatsupdocker:
    image: fmartinou/whats-up-docker:6.6.0
    container_name: whatsupdocker
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - 3200:3000
    deploy:
      replicas: $whatsupdocker  
    restart: unless-stopped
    networks:
      - observability

  tailscale:
    privileged: true
    container_name: tailscale
    image: tailscale/tailscale:v1.80.0
    volumes:
      - tailscale_data:/var/lib/tailscale
      - /dev/net/tun:/dev/net/tun                     # Required for tailscale to work
    cap_add:                                               # Required for tailscale to work
      - net_admin
      - sys_module
    networks:
      - observability
    command: tailscaled
    environment:
        - TS_AUTHKEY=${TS_AUTHKEY}
        - TS_USERSPACE=true
        - TS_STATE_DIR=/var/lib/tailscale
        - TS_SOCKET=/var/run/tailscale/tailscaled.sock
        - TS_HOSTNAME=${TS_HOSTNAME:-observindocker}
    restart: unless-stopped
    logging: *default-logging 
    deploy:
      replicas: $tailscale
    
  zerotier:
    image: zerotier/zerotier:1.14.0
    container_name: zerotier
    restart: always
    volumes:
      - zerotier_data:/var/lib/zerotier-one
      # - ./etc/zerotier/zerotier-one:/var/lib/zerotier-one
      # - ./etc/zerotier/config:/data/config
      # - ./service.d/zerotier:/service.d
    cap_drop:
      - NET_RAW
      - NET_ADMIN
      - SYS_ADMIN
    devices:
      - /dev/net/tun
    # environment:
    #   - ZEROTIER_API_SECRET=${ZEROTIER_API_SECRET}
    #   - ZEROTIER_IDENTITY_PUBLIC=${ZEROTIER_IDENTITY_PUBLIC}
    #   - ZEROTIER_IDENTITY_SECRET=${ZEROTIER_IDENTITY_SECRET}
    command: "${ZEROTIER_NETWORK_ID}"
    networks:
      - observability
    logging: *default-logging 
    deploy:
      replicas: $zerotier

  vmalert:
    container_name: vmalert
    image: victoriametrics/vmalert:v1.111.0
    # depends_on:
    #   - "victoriametrics"
    #   - "alertmanager"
    ports:
      - 8880:8880
    volumes:
      - ./etc/vmalert/alerts.yml:/etc/alerts/alerts.yml
      - ./etc/vmalert/alerts-health.yml:/etc/alerts/alerts-health.yml
      - ./etc/vmalert/alerts-vmagent.yml:/etc/alerts/alerts-vmagent.yml
      - ./etc/vmalert/alerts-vmalert.yml:/etc/alerts/alerts-vmalert.yml
    command:
      - "--datasource.url=http://victoriametrics:8428/"
      - "--remoteRead.url=http://victoriametrics:8428/"
      - "--remoteWrite.url=http://victoriametrics:8428/"
      - "--notifier.url=http://alertmanager:9093/"
      - "--rule=/etc/alerts/*.yml"
      # display source of alerts in grafana
      - "--external.url=http://127.0.0.1:3000" #grafana outside container
      # when copypaste the line be aware of '$$' for escaping in '$expr'
      - '--external.alert.source=explore?orgId=1&left={"datasource":"VictoriaMetrics","queries":[{"expr":{{$$expr|jsonEscape|queryEscape}},"refId":"A"}],"range":{"from":"now-1h","to":"now"}}'
    networks:
      - observability
    restart: always
    logging: *default-logging 
    deploy:
      replicas: $vmalert

  pushgateway:
    image: prom/pushgateway:v1.11.0
    container_name: pushgateway
    restart: always
    ports:
      - 9091:9091
    networks:
      - observability
    logging: *default-logging 
    deploy:
      replicas: $pushgateway

  caddy:
    image: caddy:2.9.1
    container_name: caddy
    ports:
      - "3000:3000"
      - "9090:9090"
      - "9093:9093"
      - "9091:9091"
      - "3100:3100"
      - 9115:9115
      - 9001:9001
      - 9000:9000
      - 9428:9428
      - 9089:9089
      - 8429:8429
      - 9080:9080
      - 9100:9100
      - '8088:8080'
    volumes:
      - ./caddy:/etc/caddy
    environment:
      - ADMIN_USER=${ADMIN_USER:-admin}
      - ADMIN_PASSWORD=${ADMIN_PASSWORD:-admin}
      - ADMIN_PASSWORD_HASH=${ADMIN_PASSWORD_HASH:-JDJhJDE0JE91S1FrN0Z0VEsyWmhrQVpON1VzdHVLSDkyWHdsN0xNbEZYdnNIZm1pb2d1blg4Y09mL0ZP}
    restart: always
    networks:
      - observability
    logging: *default-logging 
    deploy:
      replicas: $caddy

  certbot:
    image: certbot/certbot:v3.1.0
    container_name: certbot
    volumes:
      - ./nginx/certbot/conf:/etc/letsencrypt
      - ./nginx/certbot/www:/var/www/certbot
    command: certonly --webroot --webroot-path=/var/www/certbot --email your_email@example.com --agree-tos --no-eff-email -d your_domain.com
    logging: *default-logging
    deploy:
      replicas: $certbot

  angie_init_htpasswd:
    image: mhenry07/apache2-utils
    container_name: angie_init_htpasswd
    env_file:
      - path: ./.env
        required: true # default
      - path: ./.env.override
        required: false
    command: sh -c "[ -f /etc/angie_data/.htpasswd ] && { echo 'File /etc/angie_data/.htpasswd exists'; exit 1; } || mkdir -p /etc/angie_data && htpasswd -Bbc /etc/angie_data/.htpasswd ${angieadmin:-admin} ${angiepassword:-admin}"
    volumes:
      - angie_data:/etc/angie_data
    logging: *default-logging
    deploy:
      replicas: $angie_init_htpasswd

  angie_init_cert:
    image: runalsh/angie:1.8.1
    container_name: angie_init_cert
    command: sh -c "[ -f /etc/angie_data/privkey.pem ] && { echo 'File /etc/angie_data/privkey.pem exists'; exit 1; } || mkdir -p /etc/angie_data && apk add --no-cache openssl && openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout /etc/angie_data/privkey.pem -out /etc/angie_data/certificate.pem -subj '/CN=your_domain.com'"
    volumes:
      - angie_data:/etc/angie_data
    logging: *default-logging
    deploy:
      replicas: $angie_init_cert

  angie:
    image: runalsh/angie:1.8.1
    container_name: angie
    depends_on:
      - angie_init_htpasswd
      - angie_init_cert
    ports:
      - 80:80
      - 443:443
    volumes:
      - ./etc/angie/angie.conf:/etc/angie/angie.conf:ro
      - angie_data:/etc/angie_data:ro
    restart: always
    networks:
      - observability
    logging: *default-logging
    deploy:
      replicas: $angie

  minio:
    image: minio/minio:RELEASE.2024-05-28T17-19-04Z
    container_name: minio
    volumes:
      - minio_data:/data
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=admin123
      - MINIO_PROMETHEUS_AUTH_TYPE="public"
      - MINIO_LOGGER_WEBHOOK_ENABLE_target1="on"
      - MINIO_LOGGER_WEBHOOK_AUTH_TOKEN_target1="token"
      - MINIO_LOGGER_WEBHOOK_ENDPOINT_target1=http://minio:8080/minio/logs
      - MINIO_COMPRESSION_ENABLE="on"
      - MINIO_COMPRESSION_EXTENSIONS=".txt,.log,.csv,.json,.tar,.xml,.bin"
      - MINIO_COMPRESSION_MIME_TYPES="text/*,application/json,application/xml"
      - MINIO_ACCESS_KEY=s3accesskey
      - MINIO_SECRET_KEY=s3accesskey
    entrypoint:
      - sh
      - -euc
      - |
        mkdir -p /data/loki-data && \
        mkdir -p /data/loki-ruler && \
        mkdir -p /data/thanos-data && \
        minio server /data --console-address ":9001"
    restart: unless-stopped
    ports:
      - 9000:9000
      - 9001:9001
    networks:
      - observability
    logging: *default-logging
    deploy:
      replicas: $minio 

  keycloak:
    image: keycloak/keycloak:26.1.0
    container_name: keycloak
    environment:
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: admin
      DB_VENDOR: postgres
      DB_ADDR: postgres
      DB_DATABASE: keycloak
      DB_USER: keycloak
      DB_PASSWORD: password
      KC_HEALTH_ENABLED: 'true'
      KC_METRICS_ENABLED: 'true'
    command: "start-dev"  
    ports:
      - '8088:8080'
    restart: always
    networks:
      - observability      
    logging: *default-logging 
    deploy:
      replicas: $keycloak

  postgres:
    container_name: postgres
    image: postgres:17.2-alpine
    environment:
      POSTGRES_DB: keycloak
      POSTGRES_USER: keycloak
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - '5432:5432'
    networks:
      - observability
    restart: always
    logging: *default-logging 
    deploy:
      replicas: $postgres

  blackbox-exporter:
    container_name: blackbox-exporter
    image: bitnami/blackbox-exporter:0.25.0
    volumes:
      - ./etc/blackbox:/etc/blackbox:ro
    command:
      - '--config.file=/etc/blackbox/blackbox.yml'
    ports:
      - '9115:9115'
    restart: always
    networks:
      - observability
    logging: *default-logging 
    deploy:
      replicas: $blackboxexporter

  log-generator-apache:
    container_name: log-generator-apache
    # build:
    #   context: ./log-generator
    #   dockerfile: Dockerfile
    #   tags:
    #     - "runalsh/log-generator:latest"
    image: runalsh/log-generator:latest
    volumes:
      - ./etc/log-generator:/etc/log-generator:ro
      - log-generator_data:/var/log/log-generator-apache
    networks:
      - observability    
    logging: *default-logging
    deploy:
      replicas: $loggeneratorapache

  flog:
    container_name: flog
    image: mingrammer/flog:0.4.3
    command:
      - --loop
      - --format=json
      - --number=10
      - --delay=100ms
      - --output=/var/log/flog/generated-logs.txt
      - --overwrite
      - --type=log
    networks:
      - observability
    volumes:
      - flog_data:/var/log/flog
    logging: *default-logging
    deploy:
      replicas: $flog

  telemetrygen:
    container_name: telemetrygen
    image: otel/opentelemetry-collector:0.119.0
    ports:
      - "1888:1888"   # pprof extension
      - "8888:8888"   # Prometheus metrics exposed by the collector
      - "8889:8889"   # Prometheus exporter metrics
      - "13133:13133" # health_check extension
      - "9411"   # Zipkin receiver
      - "4317:4317"        # OTLP gRPC receiver
      - "4318:4318" # OTLP/HTTP receiver
      - "55680:55679" # zpages extension
      # - "3200:3100"     # Loki/Logql HTTP receiver
      # - "3201:3200"     # Loki/Logql gRPC receiver
      # - "8088:8088"     # Splunk HEC receiver
      # - "5514:5514"     # Syslog TCP Rereceiverceiver
      # - "24224:24224"   # Fluent Forward receiver
      # - "4317:4317"     # OTLP gRPC receiver
      # - "4318:4318"     # OTLP HTTP receiver
      # - "14250:14250"   # Jaeger gRPC receiver
      # - "14268:14268"   # Jaeger thrift HTTP receiver
      # - "9411:9411"     # Zipkin Trace receiver
      # - "11800:11800"   # Skywalking gRPC receiver
      # - "12800:12800"   # Skywalking HTTP receiver
      # - "8086:8086"     # InfluxDB Line proto HTTP
    volumes:
      - ./etc/telemetrygen/config.yaml:/etc/otel-collector-config.yaml # https://opentelemetry.io/docs/collector/configuration/
    networks:
      - observability
    deploy:
      replicas: $telemetrygen

  zipkin:
    image: openzipkin/zipkin:3.4.0
    container_name: zipkin
    ports:
      - '9411:9411'
    networks:
      - observability    
    deploy:
      replicas: $zipkin

  jaeger:
    image: jaegertracing/all-in-one:1.66.0
    container_name: jaeger
    ports:
      - '16686:16686'
    networks:
      - observability
    deploy:
      replicas: $jaeger  

  dockerproxy:
    image: tecnativa/docker-socket-proxy:0.3.0
    restart: always
    container_name: dockerproxy
    networks:
      - observability
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    ports:
      - 2375:2375  
    environment:
      LOG_LEVEL: info
      # 0 to revoke access / 1 to grant access.
      ## Granted by Default
      EVENTS: 1
      PING: 1
      VERSION: 1
      ## Revoked by Default
      # Security critical
      AUTH: 0
      SECRETS: 0
      # Not always needed
      BUILD: 0
      COMMIT: 0
      CONFIGS: 0
      CONTAINERS: 1 # Traefik, portainer, etc.
      DISTRIBUTION: 0
      EXEC: 0
      IMAGES: 1 # Portainer
      INFO: 1 # Portainer
      NETWORKS: 1 # Portainer
      NODES: 0
      PLUGINS: 0
      SERVICES: 1 # Portainer
      SESSION: 0
      SWARM: 0
      SYSTEM: 0
      TASKS: 1 # Portainer
      VOLUMES: 1 # Portainer
    deploy:
      replicas: $dockerproxy  

  portainer:
    image: portainer/portainer-ce:2.26.0-alpine
    restart: unless-stopped
    container_name: portainer
    networks:
      - observability
    command: ["-H", "tcp://dockerproxy:2375", "--tlsskipverify", "--admin-password", "$$2y$$05$$kvClGmhatZIlunYz332q2ebJKEFSeEdwwRL2BDnbfebJvOHC7GUTy"] #admin : admin
    healthcheck:
      test: "wget --no-verbose --tries=1 --spider --no-check-certificate http://localhost:9000 || exit 1"
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 20s
    ports:
      - 9000:9000
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - portainer_data:/data
    depends_on:
      - dockerproxy
    deploy:
      replicas: $portainer  

  influxdb:
    image: influxdb:2.7.6-alpine
    container_name: influxdb
    restart: unless-stopped
    environment:
      - INFLUXDB_CONFIG_PATH=/etc/influxdb/influxdb.conf
      - INFLUXDB_DB=gatling
    volumes:
      - ./etc/influxdb:/etc/influxdb
      - influx_data:/var/lib/influxdb
    ports:
      - 8086:8086
      - 2004:2003
    networks:
      - observability  
    deploy:
      replicas: $influxdb  

  thanos-sidecar-1:
        image: quay.io/thanos/thanos:v0.37.2
        volumes:
            - ./etc/thanos/:/etc/thanos/
            - prometheus_data:/var/lib/prometheus
        command:
            - 'sidecar'
            - '--tsdb.path=/var/lib/prometheus'
            - '--prometheus.url=http://prometheus:9090'
            - '--grpc-address=0.0.0.0:10091'
            - '--http-address=0.0.0.0:10902'
            - '--objstore.config-file=/etc/thanos/bucket_config.yaml'
        depends_on:
            - prometheus
            - minio
        restart: always
        networks:
          - observability  
        deploy:
          replicas: $thanos   

  thanos-query-frontend:
        image: quay.io/thanos/thanos:v0.37.2
        command:
            - 'query-frontend'
            - '--http-address=0.0.0.0:10901'
            - '--query-frontend.downstream-url=http://thanos-querier:10902'
        ports:
            - 10901:10901
        depends_on:
            - thanos-querier
        restart: always
        networks:
          - observability  
        deploy:
          replicas: $thanos     

  thanos-querier:
        image: quay.io/thanos/thanos:v0.37.2
        command:
            - 'query'
            - '--grpc-address=0.0.0.0:10091'
            - '--http-address=0.0.0.0:10902'
            - '--query.replica-label=replica'
            - '--store=thanos-sidecar-1:10091'
            # - '--store=thanos-sidecar-2:10091'
            # - '--store=thanos-sidecar-3:10091'
            # - '--store=thanos-sidecar-4:10091'
            - '--store=thanos-store-gateway:10091'
            - '--store=thanos-ruler:10091'
        ports:
            - 10902:10902
        depends_on:
            - thanos-sidecar-1
        restart: always
        networks:
          - observability
        deploy:
          replicas: $thanos       

  thanos-store-gateway:
        image: quay.io/thanos/thanos:v0.37.2
        volumes:
            - ./etc/thanos/:/etc/thanos/
        command:
            - 'store'
            - '--grpc-address=0.0.0.0:10091'
            - '--http-address=0.0.0.0:10902'
            - '--data-dir=/tmp/thanos/store'
            - '--objstore.config-file=/etc/thanos/bucket_config.yaml'
        depends_on:
            - minio
        restart: always
        networks:
          - observability
        deploy:
          replicas: $thanos       

  thanos-compactor:
        image: quay.io/thanos/thanos:v0.37.2
        volumes:
            - ./etc/thanos/:/etc/thanos/
        command:
            - 'compact'
            - '--log.level=debug'
            - '--data-dir=/data'
            - '--objstore.config-file=/etc/thanos/bucket_config.yaml'
            - '--wait'
        depends_on:
            - minio
        restart: always
        networks:
          - observability
        deploy:
          replicas: $thanos       

  thanos-ruler:
        image: quay.io/thanos/thanos:v0.37.2
        volumes:
            - ./etc/thanos/:/etc/thanos/
        command:
            - 'rule'
            - '--grpc-address=0.0.0.0:10091'
            - '--http-address=0.0.0.0:10902'
            - '--log.level=debug'
            - '--data-dir=/data'
            - '--eval-interval=15s'
            - '--rule-file=/etc/thanos/*.rules.yaml'
            - '--alertmanagers.url=http://alertmanager:9093'
            - '--query=thanos-querier:10902'
            - '--objstore.config-file=/etc/thanos/bucket_config.yaml'
            - "--label=ruler_cluster=\"vegas\""
            - "--label=ruler_replica=\"r1\""
        ports:
            - 10903:10902
        depends_on:
            - minio
            - thanos-querier
        restart: always
        networks:
          - observability
        deploy:
          replicas: $thanos       

  thanos-bucket-web:
        image: quay.io/thanos/thanos:v0.37.2
        volumes:
            - ./etc/thanos/:/etc/thanos/
        command:
            - 'tools'
            - 'bucket'
            - 'web'
            - '--http-address=0.0.0.0:10902'
            - '--log.level=debug'
            - '--objstore.config-file=/etc/thanos/bucket_config.yaml'
            - '--refresh=5m'
            - '--timeout=2m'
            - '--label=replica'
        ports:
            - 10904:10902
        depends_on:
            - minio
        restart: always
        networks:
          - observability
        deploy:
          replicas: $thanos       

  clickhouse:
    image: clickhouse/clickhouse-server:25.1-alpine
    container_name: clickhouse
    environment:
      - CLICKHOUSE_USER=clickhouseuser
      - CLICKHOUSE_PASSWORD=clickhousepasswd
    ports:
      - 8123:8123
    networks:
      - observability
    deploy:
      replicas: $clickhouse 

  qryn:
    image: qxip/qryn:3.2.25
    container_name: qryn
    ports:
      - "3101:3100"
    volumes:
      - clickhouse_data:/var/lib/clickhouse
    environment:
      - CLICKHOUSE_SERVER=clickhouse
      - CLICKHOUSE_PORT=8123
      - CLICKHOUSE_PROTO="https"
      - CLICKHOUSE_AUTH=clickhouseuser:clickhousepasswd
      - CLICKHOUSE_DB=qryn
    networks:
      - observability
    deploy:
      replicas: $qryn 

  vector:
    image: timberio/vector:0.44.0-alpine
    container_name: vector
    restart: unless-stopped
    volumes:
      - ./etc/vector/vector.toml:/etc/vector/vector.toml:ro
    networks:
      - observability  
    deploy:
      replicas: $vector 

  cloki:
    image: qxip/cloki:3.2.25
    container_name: cloki
    ports:
      - "3102:3100"
    environment:
      - CLICKHOUSE_SERVER=clickhouse
      - CLICKHOUSE_DB=cloki
      - CLICKHOUSE_AUTH=clickhouseuser:clickhousepasswd
    networks:
      - observability  
    deploy:
      replicas: $cloki

  pastash:
    image: qxip/pastash-loki
    container_name: pastash
    volumes:
      - ./etc/cloki/pastash.json:/config/loki.conf
      - /var/log:/var/log:ro
    networks:
      - observability  
    deploy:
      replicas: $cloki 

  mimir:
    image: grafana/mimir:2.15.0
    command: ["-config.file=/etc/mimir.yaml"]
    hostname: mimir
    container_name: mimir
    volumes:
      - ./etc/mimir/mimir.yaml:/etc/mimir.yaml
      - ./etc/mimir/alertmanager-fallback-config.yaml:/etc/alertmanager-fallback-config.yaml
      - mimir_data:/data
    networks:
      - observability  
    deploy:
      replicas: $mimir 










